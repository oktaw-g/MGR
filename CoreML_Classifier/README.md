# ğŸ“¦ Zero-Shot Image Classifier Evaluator (CreateML + CoreML)

This project lets you **evaluate** any pre-trained CoreML model (`.mlmodel` or `.mlpackage`) on a **custom dataset**, calculating important metrics like accuracy, precision, recall, and F1 score â€” and generates an **HTML report** with confusion matrix and random sample visualizations!

---

## ğŸ“‚ Project Structure

```
/path/to/your/models/
    â”œâ”€â”€ model1.mlmodel
    â”œâ”€â”€ model2.mlmodel
    â””â”€â”€ model3.mlpackage

/path/to/your/dataset/
    â”œâ”€â”€ ClassA/
    â”‚   â”œâ”€â”€ image1.jpg
    â”‚   â”œâ”€â”€ image2.jpg
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ ClassB/
    â”‚   â”œâ”€â”€ image1.jpg
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...

/path/to/output/folder/
    â””â”€â”€ (Generated reports, confusion matrix CSVs, sample images)
```

---

## âš™ï¸ Requirements

- macOS (Monterey or newer recommended)
- Xcode with CreateML and CoreML frameworks installed
- Swift 5.7+ project (macOS Command Line App template works best)

---

## ğŸš€ How to Use

1. Open **Xcode** and create a **Command Line macOS App** project.
2. Add the provided `main.swift` code into your project.
3. Update the following paths at the top of the script:
   ```swift
   let modelFolderPath = "/path/to/your/models"
   let datasetFolderPath = "/path/to/your/dataset"
   let outputFolderPath = "/path/to/output/folder"
   ```
4. Build and Run the app.
5. The script will display a list of models.  
   **Enter the number** of the model you want to evaluate.

6. After evaluation, you will find:
   - `confusion_matrix.csv`
   - random sample images with predictions
   - a complete `report.html` file in your output folder.

---

## ğŸ› ï¸ Configurable Options

- **Batch Size**  
  You can adjust how many images are evaluated in parallel by changing:
  ```swift
  let batchSize = 8
  ```

- **Number of Samples Shown**  
  You can control how many random samples are included in the HTML report:
  ```swift
  saveRandomPredictions(samples: evaluatedSamples, outputFolder: samplesFolder, count: 3)
  ```

---

## ğŸ“ˆ Metrics Calculated

- Accuracy
- Precision
- Recall
- F1 Score

All based on your provided dataset and model predictions!

---

## ğŸ“‘ Example Outputs

After evaluation, you will have:

- âœ… An **HTML report** summarizing metrics
- âœ… A **Confusion Matrix** as `.csv`
- âœ… **3 random images** showing true and predicted labels

---

## ğŸ¯ Goal

The goal is to **compare** different models **before and after training** on your dataset using consistent metrics and sample visualizations.

---

# ğŸ”¥ Happy Evaluating!


